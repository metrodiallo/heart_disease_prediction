# -*- coding: utf-8 -*-
"""heart-disease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xlZK4HYF-rVf5zHYsI6rrmCkxa2gcf8H

# Heart Disease Prediction model

**Problem:**

In this project, we delve into a dataset encapsulating various health metrics from heart patients, including age, blood pressure, heart rate, and more. Our goal is to develop a predictive model capable of accurately identifying individuals with heart disease. Given the grave implications of missing a positive diagnosis, our primary emphasis is on ensuring that the model identifies all potential patients, making recall for the positive class a crucial metric
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from scipy.stats import boxcox
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score


# %matplotlib inline

"""# 1.Data Collection"""

df=pd.read_csv('/content/heart.csv')

df.info()

"""# Dataset Description

## Context
This dataset dates back to 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains **76 attributes**, including the predicted attribute. However, all published experiments use a subset of **14 attributes**. The "target" field indicates the presence of heart disease in the patient, where:
- **0** = No disease
- **1** = Disease

## Content

### Attribute Information
1. **age**: Age of the patient
2. **sex**: Gender of the patient
3. **chest pain type**: (4 values)
4. **resting blood pressure**: Blood pressure while at rest
5. **serum cholestoral**: Cholesterol level in mg/dl
6. **fasting blood sugar**: > 120 mg/dl (1 = true; 0 = false)
7. **resting electrocardiographic results**: Values 0, 1, 2
8. **maximum heart rate achieved**
9. **exercise induced angina**: (1 = yes; 0 = no)
10. **oldpeak**: ST depression induced by exercise relative to rest
11. **slope**: The slope of the peak exercise ST segment
12. **number of major vessels**: (0-3) colored by fluoroscopy
13. **thal**:  
    - 0 = Normal  
    - 1 = Fixed defect  
    - 2 = Reversible defect

   ## Dataset Link
   
https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download

"""

df.shape

df.isnull().sum()

"""# No Null rows are present and all the columns are numeric .Lets dig deeper."""

df.describe()

df.head()

for i in ['sex','cp','fbs','restecg','exang','slope','ca','thal','target']:
    df[i] = df[i].astype('object')

print(df.dtypes)

# Define the continuous features
continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

# Identify the features to be converted to object data type
categorical_features = [feature for feature in df.columns if feature not in continuous_features]

# Convert the identified features to object data type
df[categorical_features] = df[categorical_features].astype('object')

df_continuous=df[continuous_features]
df_categorical=df[categorical_features]

df_continuous.describe()

"""# 2. Data Cleaning Steps

## Identify Row with duplicate data
"""

duplicate = df[df.duplicated()]
print(f"Duplicate rows present in the dataset are: {duplicate}")
duplicate.head()

df=df.drop_duplicates()

df.describe()

df.describe(include='object')

df.isnull().sum()

for i in df_continuous:
    plt.hist(df_continuous[i])
    plt.title(i)
    plt.show()

for i in df_continuous.columns:
    skewness=3*(df[i].mean()-df[i].median())/df[i].std()
    print(f'Skewness of column {i} is {skewness}')

for i in df_categorical.columns:
    print(df_categorical[i].value_counts())

for i, col in enumerate(df_continuous.columns):
    x = i // 3
    y = i % 3
    print(x,y)

# Filter out continuous features for the univariate analysis
df_continuous = df[continuous_features]

# Set up the subplot
fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop to plot histograms for each continuous feature
for i, col in enumerate(df_continuous.columns):
    x = i // 3
    y = i % 3
    values, bin_edges = np.histogram(df_continuous[col],
                                     range=(np.floor(df_continuous[col].min()), np.ceil(df_continuous[col].max())))

    graph = sns.histplot(data=df_continuous, x=col, bins=bin_edges, kde=True, ax=ax[x, y],
                         edgecolor='none', color='red', alpha=0.6, line_kws={'lw': 3})
    ax[x, y].set_xlabel(col, fontsize=15)
    ax[x, y].set_ylabel('Count', fontsize=12)

ax[1,2].axis('off')
plt.suptitle('Distribution of Continuous Variables', fontsize=20)
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()

for i, x_feature in enumerate(continuous_features):
    for j, y_feature in enumerate(continuous_features):
        if x_feature != y_feature:  # Avoid plotting x vs. x
            plt.figure(figsize=(8, 6))
            sns.scatterplot(
                data=df,
                x=x_feature,
                y=y_feature,
                hue='target',
                sizes=(20, 200),  # Adjust bubble size range
                legend=True,
                palette='viridis'
            )
            plt.title(f'Scatterplot: {x_feature} vs {y_feature}')
            plt.show()

# Set up the subplot for a 4x2 layout
fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(15, 18))

# Loop to plot bar charts for each categorical feature in the 4x2 layout
for i, col in enumerate(categorical_features):
    row = i // 2
    col_idx = i % 2

    # Calculate frequency percentages
    value_counts = df[col].value_counts(normalize=True).mul(100).sort_values()

    # Plot bar chart
    value_counts.plot(kind='barh', ax=ax[row, col_idx], width=0.8, color='red')

    # Add frequency percentages to the bars
    for index, value in enumerate(value_counts):
        ax[row, col_idx].text(value, index, str(round(value, 1)) + '%', fontsize=15, weight='bold', va='center')

    ax[row, col_idx].set_xlim([0, 95])
    ax[row, col_idx].set_xlabel('Frequency Percentage', fontsize=12)
    ax[row, col_idx].set_title(f'{col}', fontsize=20)

ax[4,1].axis('off')
plt.suptitle('Distribution of Categorical Variables', fontsize=22)
plt.tight_layout()
plt.subplots_adjust(top=0.95)
plt.show()

# Set color palette
sns.set_palette(['#ff826e', 'red'])

# Create the subplots
fig, ax = plt.subplots(len(continuous_features), 2, figsize=(15,15), gridspec_kw={'width_ratios': [1, 2]})

# Loop through each continuous feature to create barplots and kde plots
for i, col in enumerate(continuous_features):
    # Barplot showing the mean value of the feature for each target category
    graph = sns.barplot(data=df, x="target", y=col, ax=ax[i,0])

    # KDE plot showing the distribution of the feature for each target category
    sns.kdeplot(data=df[df["target"]==0], x=col, fill=True, linewidth=2, ax=ax[i,1], label='0')
    sns.kdeplot(data=df[df["target"]==1], x=col, fill=True, linewidth=2, ax=ax[i,1], label='1')
    ax[i,1].set_yticks([])
    ax[i,1].legend(title='Heart Disease', loc='upper right')

    # Add mean values to the barplot
    for cont in graph.containers:
        graph.bar_label(cont, fmt='         %.3g')

# Set the title for the entire figure
plt.suptitle('Continuous Features vs Target Distribution', fontsize=22)
plt.tight_layout()
plt.show()

# Implementing one-hot encoding on the specified categorical features
df_encoded = pd.get_dummies(df, columns=['cp', 'restecg', 'thal'], drop_first=True)

# Convert the rest of the categorical variables that don't need one-hot encoding to integer data type
features_to_convert = ['sex', 'fbs', 'exang', 'slope', 'ca', 'target']
for feature in features_to_convert:
    df_encoded[feature] = df_encoded[feature].astype(int)

df_encoded.dtypes

"""# Model Building"""

from sklearn.model_selection import train_test_split

# Define the features (X) and the output labels (y)
X = df_encoded.drop('target', axis=1)
y = df_encoded['target']

X

y

# Step 1: Split data into training (70%) and temporary (30%) datasets
train_data, temp_data = train_test_split(df_encoded, test_size=0.3, random_state=42)

# Step 2: Split the temporary dataset equally for validation (15%) and test (15%) datasets
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

X_train, y_train = train_data.drop("target", axis=1), train_data['target']
X_val, y_val = val_data.drop("target", axis=1), val_data['target']
X_test, y_test = test_data.drop("target", axis=1), test_data['target']

"""##Logistic Regression Implementation"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Instantiate and train Logistic Regression
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)

# Evaluate Logistic Regression on the test set
y_pred_lr = lr_model.predict(X_test)

# Print evaluation metrics
print("Logistic Regression Performance:")
print(classification_report(y_test, y_pred_lr))
print(f"Accuracy: {accuracy_score(y_test, y_pred_lr)}")

"""##Deep Neural Network (DNN) Implementation with PyTorch"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from torch.utils.data import DataLoader, TensorDataset

# Ensure boolean columns are cast to integers
df = df.copy()
bool_cols = df.select_dtypes(include=['bool']).columns
df[bool_cols] = df[bool_cols].astype(int)

# Separate features and target
X = df.drop(columns='target')
y = df['target']

# Split data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert all columns to numeric (if not already numeric)
X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)
X_val = X_val.apply(pd.to_numeric, errors='coerce').fillna(0)
X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values.astype(np.float32), dtype=torch.float32).view(-1, 1)

X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val.values.astype(np.float32), dtype=torch.float32).view(-1, 1)

X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values.astype(np.float32), dtype=torch.float32).view(-1, 1)


print("Data successfully converted to tensors!")

# Define the DNN model
class HeartDiseaseModel(nn.Module):
    def __init__(self):
        super(HeartDiseaseModel, self).__init__()
        self.fc1 = nn.Linear(X_train.shape[1], 32)
        self.fc2 = nn.Linear(32, 16)
        self.fc3 = nn.Linear(16, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.sigmoid(self.fc3(x))
        return x

# Instantiate the model
dnn_model = HeartDiseaseModel()

# Define loss and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(dnn_model.parameters(), lr=0.001)

# Train the model
epochs = 50
batch_size = 32
train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)

for epoch in range(epochs):
    dnn_model.train()
    epoch_loss = 0.0
    for batch in train_loader:
        X_batch, y_batch = batch
        optimizer.zero_grad()
        outputs = dnn_model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    # Evaluate on the validation set
    dnn_model.eval()
    with torch.no_grad():
        val_outputs = dnn_model(X_val_tensor)
        val_loss = criterion(val_outputs, y_val_tensor).item()

    print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss / len(train_loader):.4f}, Validation Loss: {val_loss:.4f}")

# Evaluate the DNN on the test set
dnn_model.eval()
with torch.no_grad():
    y_pred_dnn = (dnn_model(X_test_tensor).numpy() > 0.5).astype(int)
    y_test_dnn = y_test_tensor.numpy()

# Print evaluation metrics
print("Deep Neural Network Performance:")
print(classification_report(y_test_dnn, y_pred_dnn))
print(f"Accuracy: {accuracy_score(y_test_dnn, y_pred_dnn):.4f}")

"""### Observations:

1. **Logistic Regression Performance:**
   - **Precision**:
     - Class 0: 0.95
     - Class 1: 0.75
     - Class 0 has a high precision, while Class 1 has a lower precision.
   - **Recall**:
     - Class 0: 0.78
     - Class 1: 0.95
     - Class 1 has a much higher recall, indicating better identification of Class 1 instances.
   - **F1-Score**:
     - Class 0: 0.86
     - Class 1: 0.84
     - Both classes have similar F1-scores, showing balanced performance for both classes.
   - **Accuracy**: 85%
     - The logistic regression model shows strong overall accuracy.

2. **Deep Neural Network (DNN) Performance:**
   - **Precision**:
     - Class 0: 0.81
     - Class 1: 0.70
     - The DNN model has lower precision than logistic regression, especially for Class 1.
   - **Recall**:
     - Class 0: 0.74
     - Class 1: 0.78
     - The DNN is better at identifying Class 1 but struggles with Class 0.
   - **F1-Score**:
     - Class 0: 0.77
     - Class 1: 0.74
     - The F1-scores are lower for the DNN compared to logistic regression but are relatively balanced between the classes.
   - **Accuracy**: 75%
     - The DNN model shows lower accuracy than logistic regression, indicating less overall effectiveness on this dataset.

3. **Comparison:**
   - **Accuracy**: Logistic regression outperforms the DNN with an accuracy of 85% vs. 75%.
   - **Precision and Recall**: Logistic regression shows higher precision and balanced recall, while the DNN has a lower precision but a higher recall for Class 1.
   - **F1-Score**: Logistic regression has a more balanced F1-score, while the DNN has slightly lower F1-scores for both classes.

### Conclusion:
The logistic regression model performs better than the deep neural network on this dataset, achieving higher accuracy and more balanced precision and recall. The DNN might need further tuning or a more complex dataset to outperform logistic regression.

##Prediction Using Best Performing Model(logistic regression )
"""

# Example (assuming you've identified the missing features and their values)
X_new = [[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 2, 3, 0, 0, 0, 0, 0]]

# Make prediction using the Logistic Regression model
y_pred_new = lr_model.predict(X_new)

# Print the prediction result with a descriptive message
if y_pred_new[0] == 1:
    print("Prediction: Patient have Heart Disease ", y_pred_new)
else:
    print("Prediction:  Patient have No Heart Disease ", y_pred_new)